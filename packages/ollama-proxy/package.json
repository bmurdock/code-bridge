{
  "name": "ollama-proxy",
  "version": "0.1.0",
  "private": true,
  "type": "module",
  "description": "Ollama-compatible HTTP proxy that forwards to the VS Code LM bridge (/models, /chat).",
  "bin": {
    "ollama-proxy": "dist/server.js"
  },
  "scripts": {
    "build": "tsc -p .",
    "dev": "ts-node-esm src/server.ts",
    "dev:mock": "ts-node-esm src/mock/mock-bridge.ts",
    "lint": "eslint \"src/**/*.ts\"",
    "test": "vitest run --passWithNoTests"
  },
  "dependencies": {
    "fastify": "^4.28.1",
    "zod": "^3.23.8"
  },
  "devDependencies": {
    "@types/node": "^20.12.7",
    "typescript": "^5.5.4",
    "ts-node": "^10.9.2"
  }
}

